\documentclass[lineaire_algebra_oplossingen.tex]{subfiles}
\begin{document}

\chapter{Theorie Hoofdstuk 5}
\section{Bewijzen uit de cursus}

\subsection{Stelling 5.2 p 177}
Zij $A\in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
Een getal $\lambda\in\mathbb{R}$ is een eigenwaarde van $A$.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$\lambda$ is een nulpunt van de karakteristieke veelterm $det(X\mathbb{I}_n - A)$ van $A$
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
Omdat $\lambda$ een eigenwaarde is van $A$ bestaat er een (eigen)vector $v$ zodat de volgende bewering geldt\footnote{Zie Definitie 5.1 p 177}.
\[
A\cdot v = \lambda v
\]
We weten dat $\lambda v =  \lambda \mathbb{I}_n \cdot v$ geldt en dat de matrixvermenigvuldiging distributief is als ze bepaald is\footnote{Zie Eigenschappen 1.22 b}.
\[
A\cdot v - \lambda \mathbb{I}_n \cdot v = \vec{0} = (A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
Omdat $v$ per definitie geen nulvector is moet de determinant van $(A-\lambda\mathbb{I}_n)$ nul zijn opdat opdat $(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}$ geldt.
Dit is zo omdat $(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}$ een homogeen stelsel is, en homogene stelsels enkel een niet-nuloplossing hebben als de determinant van de matrix ervan nul is.

\item $\Leftarrow$\\
Als $det(A-\lambda\mathbb{I}_n) = 0$ geldt voor $\lambda$ met $v$ als eigenvector, dan heeft het volgende stelsel een niet-nuloplossing. Dit is nodig omdat eigenvectoren per definitie geen nulvectoren zijn.
\[
(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
\end{itemize}
\end{proof}

\subsection{Voorbeeld 5.4 p 178}
\subsubsection*{1)}
We zoeken nog een oplossing van de volgende vergelijking.
\[
\begin{pmatrix}
-1 & -1\\
-1 & -1\\
\end{pmatrix}
\cdot
\begin{pmatrix}
x\\y
\end{pmatrix}
=
\vec{0}
\]
De oplossingsverzameling hiervan is de volgende.
\[
V = \{-\lambda,\lambda|\lambda\in\mathbb{R}\}
\]
Als we nu $\lambda = 1$ kiezen krijgen we als eigenvector bij voorbeeld $(-1,1)$.

\subsubsection*{5)}
Voor elke eigenvector $v$ van $D$ geldt dat de afgeleide van $v$ een veelvoud is van $v$. Elke constante functie is een eigenvector en $0$ is de eigenwaarde voor die functies. Dit is gemakkelijk te zien aan de matrix van de afgeleide afbeelding.
\[
D_{\epsilon} = 
\begin{pmatrix}
0 & 1 & 0 & 0 & \cdots\\
0 & 0 & 2 & 0 & \cdots\\
0 & 0 & 0 & 3 & \cdots\\
0 & 0 & 0 & 0 & \cdots\\
\vdots &\vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\]

\subsection{Over Definitie 5.6 p 181}
Dit houdt in dat er voor $A$ een inverteerbare matrix $P$ bestaat zodat. $A = P^{-1}\cdot B\cdot P$ waarbij $B$ een diagonaalmatrix is.

\subsection{Stelling 5.7 p 181}
Zij $A \in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
$A$ is diagonaliseerbaar.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$A$ heeft een basis die volledig bestaat uit eigenvectoren van $A$.
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$
Omdat $A$ diagonaliseerbaar is bestaat er een inverteerbare matrix $P$ zodat $P^{-1}AP = \Lambda$ geldt met $\Lambda$ een diagonaalmatrix. $\Lambda$ ziet er dus als volgt uit.
\[
\Lambda =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}
\]
Nu geldt $A \cdot P = P \cdot \Lambda$ omwille van de definitie van inverteerbaarheid\footnote{Zie Definitie 1.27 p 34}.
We beschouwen $P$ nu als een rij van kolommen $P=(p_1 p_2 \cdots p_n)$
$A\cdot P$ ziet er dus als volgt uit.
\[
A\cdot P = A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n)
\]
Bovendien geldt de volgende bewering omdat $A \cdot P = P \cdot \Lambda$ geldt.
\[
A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n) = (p_1 p_2 \cdots p_n)\Lambda = (\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)
\]
Elk van de kolommen in $(\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)$ voldoet dus aan de volgende vergelijking omdat $\Lambda$ een diagonaalmatrix is.
\[
Ap_i = \lambda_ip_i
\]
\textbf{Elke kolom van $P$ is bijgevolg een eigenvector van $A$ de eigenwaarden die horen bij deze eigenvectoren staan dan in de overeenkomstige kolommen op de diagonaal van $\Lambda$.}
Nu moeten we nog bewijzen dat de kolommen van $P$ een basis vormen voor $\mathbb{R}^n$
Omdat $P$ precies $n$ kolommen bevat moeten we enkel bewijzen dat de kolommen van $P$ lineair onafhankelijk of dat ze voortbrengend zijn\footnote{Zie Stelling 3.41 p 109}.
Omdat $P$ inverteerbaar is geldt dat de determinant van $P$ niet nul is\footnote{Zie Stelling 2.4 p 59}.
Bijgevolg zijn de kolommen van $P$ lineair onafhankelijk\footnote{Stelling 2.2 p 57 en Zie Stelling 2.3 p 58} omdat $P$ rijreduceerbaar is tot een matrix zonder nulrijen.

\item $\Leftarrow$
$\mathbb{R}^n$ heeft een basis volledig bestaand uit eigenvectoren van $A$. Noem deze basis $\beta = \{\beta_1,\beta_2,...,\beta_n\}$. De vectoren uit deze basis voldoen dus aan de volgende bewering.
\[
A\cdot \beta_i = \lambda_i\beta_i
\]
$A$ beschouwen we als de matrix van een lineaire afbeelding ten opzichte van de standaard basis.
De matrix van basisverandering van de basis van de eigenvectoren naar de standaardbasis noemen we $P$.
Dit is precies de matrix waarin de eigenvectoren van $A$ (de vectoren van $\beta$) in kolommen staan.
$P^{-1}$ is nu de matrix van basisverandering van de standaardmatrix naar de basis van eigenvectoren.
$P$ is zeker inverteerbaar omdat de kolommen lineair onafhankelijk zijn (ze vormen een basis).
We weten nu dat $A = PBP^{-1}$ geldt\footnote{Zie pagina 150 voor meer uitleg.}. Vermenigvuldig nu $P$ rechts aan beide kanten.
\[
AP = PB
\]
Beschouw $P = (\beta_1 \beta_2 \cdots \beta _n)$ als een rij van kolommen $\beta_i$. Beschouw $B = (b_1 b_2 \cdots b_n)$ als een kolom van rijen $b_i$.
\[
AP = (A\beta_1 A\beta_2 \cdots A\beta_n) = (\lambda_1\beta_1 \lambda_2\beta_2 \cdots \lambda_n\beta_n) = PB
\]
De tweede gelijkheid geldt omdat $\beta_i$ eigenvectoren zijn.
De derde gelijkheid kan enkel gelden als $B$ een diagonaalmatrix is. Sterker nog, $\lambda_i$, de eigenwaarden van $A$ staan precies op de diagonaal van $P$.

\end{itemize}
\end{proof}

\subsection{Stelling 5.8 p 182}
Zij $A,B \in \mathbb{R}^{n\times n}$ vierkante matrices met $B = P^{-1}AP$ zodat $A$ en $B$ gelijkvormig zijn.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $A$ en $B$ hebben dezelfde karakteristieke veelterm.
\item $A$ en $B$ hebben dezelfde determinant.
\item $A$ en $B$ hebben hetzelfde spoor.
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.
\begin{enumerate}
\item 
\[
det(X\mathbb{I}_n - B) = det(X\mathbb{I}_n - P^{-1}AP) = det(XP^{-1}P - P^{-1}AP)
\]
Merk op dat $X$ een scalar is, geen vector.
We weten dat $XP^{-1}P = XP^{-1}\mathbb{I}_nP=P^{-1}X\mathbb{I}_nP$ geldt \footnote{Zie Eigenschap 1.22 p 32 d)}. We kunnen dus verder gaan via gelijkheden.
\[
= det(P^{-1} XI P - P^{-1}AP) = det(P^{-1}\cdot ( XI P - AP)) = det(P^{-1}\cdot ( XI- A) \cdot P))
\]
De bovenstaande twee vergelijkingen gelden omwille van de distributiviteit van de matrixvermenigvuldiging ten opzichte van de matrix optelling\footnote{Zie Eigenschap 1.22 p 32 a) en b)}. De volgende gelijkheden gelden omwille van een eigenschap dan de determinantafbeelding\footnote{Zie Stelling 2.4 p 59 3} en de commutativiteit van de vermenigvuldiging in $\mathbb{R}$.
\[
=det(P^{-1})\cdot det( XI- A)\cdot det(P) =det( XI- A)\cdot det(P^{-1}) \cdot det(P)
\]
We weten dat $det( P^{-1}) = \frac{1}{det(P)}$\footnote{Zie Gevolg 2.5 p 60} dus de volgende gelijkheid geldt ook.
\[
=det( XI- A)\cdot \frac{1}{det(P)}\cdot det(P) = det( XI- A)
\]
Dat laatste rechterlid is precies de karakteristieke vergelijking van $A$.

\item
Als $A$ gelijkvormig is met $B$ geldt de volgende gelijkheid.
\[
B = P^{-1}AP
\]
Nemen we nu de determinant van beide kanten dan bekomen we de volgende gelijkheid.
\[
det(B) = det(P^{-1}AP) = det(P^{-1})\cdot det(A)\cdot det(P)
\]
\[
= det(A)\cdot det(P^{-1})\cdot det(P) = det(A)\cdot \frac{1}{det(P)}\cdot det(P) = det(A) \cdot 1 = det(A)
\]

\item
\[
B = P^{-1}AP 
\]
\[
Tr(B) = Tr(P^{-1}A P) = Tr(P^{-1}P A) = Tr(IA) = Tr(A)
\]
Bovenstaande gelijkheden gelden omwille van een eigenschap van het spoor van een product, namelijk dat $Tr(AB) = Tr(BA)$ geldt\footnote{Zie opdracht 1.25 p 33}.
\end{enumerate}
\end{proof}

\subsection{Gevolg 5.9 p 182}
Zij $L$ een lineaire transformatie van $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
De karakteristieke veelterm, de determinant en het spoor van de matrix van $L$ zijn onafhankelijk van de gekozen basis.

\subsubsection*{Bewijs}
\begin{proof}
We moeten enkel bewijzen dat elke matrixvoorstelling van $L$ gelijkvormig is met elke andere matrixvoorstelling van $L$. De rest volgt dan uit het bewijs van de vorige stelling \footnote{Zie Stelling 5.8 p 182.}. Dit is echter al gebeurd \footnote{Zie Gevolg 4.22 p 151.}.
\end{proof}



\subsection{Stelling 5.16 p 189}
Zij $L:V\rightarrow V$ een lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$ met spectrum $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_n\}$.

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $m(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \le m(\lambda_i)$\\w
\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.
Voor elke eigenwaarde $\lambda_i$ geldt het volgende.
\begin{enumerate}
\item Elke eigenwaarde heeft minstens een algebra\"ische multipliciteit van $1$.
Anders zou het geen eigenwaarde zijn.

\item De eigenruimte van $\lambda_i$ is een vectorruimte van minstens dimensie $1$ omdat er voor elke eigenwaarde minstens $1$ eigenvector is, en dit niet de nulvector mag zijn.

\item
We weten dat de eigenruimte $E_{\lambda_i}$ van $\lambda_i$ dimensie $d(\lambda) = d$ heeft.
Er bestaat dus een basis $\beta = v_1,v_2,...,v_d$ voor $E_\lambda$ met $d$ elementen.
Omdat $\beta$ een vrij deel is van $E_\lambda$ en $V$ en omdat $E_\lambda$ een deelruimte is van V\footnote{Zie Definitie 5.14 p 188} geldt dat $\beta$ uitgebreid kan worden tot een basis van $V$\footnote{Zie Stelling 3.37 p 107}.
Ten opzichte die uitgebreide basis $\beta'$ van $V$ ziet de matrix van $L$ er als volgt uit\footnote{Zie Stelling 5.7 p 181}.
(Dit is het punt van eigenvectoren).
($L_{\beta'}^{\beta'}\cdot v_i$ moet $\lambda v_i$ zijn.)
\[
L_{\beta'}^{\beta'} = 
\begin{pmatrix}
\lambda & 0 & \cdots & 0 & \bullet & \cdots & \bullet\\
0 & \lambda & \cdots & 0 & \bullet & \cdots & \bullet\\
\vdots & \vdots & \ddots & \vdots & \vdots & &  \vdots\\
0 & 0 & \cdots & \lambda & \bullet & \cdots & \bullet\\
\vdots & \vdots & & \vdots & \vdots & \ddots & \vdots\\
0 & 0 &\cdots & 0 & \bullet & \cdots & \bullet\\
\end{pmatrix}
\]
We weten dat de karakteristieke veelterm onafhankelijk is van de gekozen basis\footnote{Zie Gevolg 5.9 p 182}. We beschouwen nu de karakteristieke veelterm van bovenstaande matrix.
De karakteristieke veelterm van deze matrix is dan van de volgende vorm. 
\[
\phi_L(X) = (X-\lambda)^dp(X)
\]
$m(\lambda)$ is minstens $d$ want $(X-\lambda)^d$ zorgt al voor multipliciteit $d$, maar in $p(X)$ kan $\lambda$ ook nog voorkomen. In symbolen:
\[
m(\lambda) \ge d = d(\lambda)
\]
\end{enumerate}
\end{proof}

\subsection{Stelling 5.18 p 190}
Zij $L:V\rightarrow V$ een lineaire transformatie van een eindigdimensionale vectorruimte $(\mathbb{R},V,+)$ en zij $\lambda_1,\lambda_2,...,\lambda_n$ verschillende eigenwaarden van $L$ met bijhorende eigenvectoren $v_1,v_2,...,v_n$.
\subsubsection*{Te Bewijzen}
$v_1,v_2,...,v_n$ zijn lineair onafhankelijke vectoren.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door inductie op $n$.\\\\
\emph{Stap 1: (basis stap)}\\
De bewering geldt voor $n=1$ want $\{v_i\}$ is lineair onafhankelijk. Dit is waar omdat $v_i$ geen nulvector is\footnote{Zie Definitie 5.3 p 178}.\\\\
\emph{Stap 2: (inductie stap)}\\
Stel dat de bewering geldt voor een bepaalde $n=k$ (inductiehypothese). We bewijzen nu dat daaruit volgt dat de bewering geldt voor $n=k+1$.
We beschouwen $k+1$ eigenwaarden met bijhorende eigenvectoren. Uit de inductiehypothese volgt dat elke deelverzameling van $k$ elementen van die eigenvectoren vrij is.
We bewijzen uit het ongerijmde dat wanneer we de $k+1$-ste eigenvector toevoegen aan die deelverzameling, de verzameling nog steeds vrij is.
We nemen dus aan dat $v_{k+1}$ lineaire afhankelijk is van $v_1,v_2,...,v_k$ en proberen tot een contradictie te komen.
\[
\exists \mu_i\in\mathbb{R} : v_{k+1} = \sum_{i=1}^k\mu_iv_i
\]
Nemen we nu van beide kanten de afbeelding $L$ dan bekomen we de volgende vergelijking.
\[
L( v_{k+1}) = L\left(\sum_{i=1}^k\mu_iv_i\right)
\]
L is lineair\footnote{Zie Lemma 4.2 p 130} en $v_i$ zijn eigenvectoren met eigenwaarden $\lambda_i$ (gegeven).
\[
L(v_{k+1}) = \sum_{i=1}^k\mu_iL(v_i) = \sum_{i=1}^k\mu_i\lambda_iv_i
\]
Bovendien is $v_{k+1}$ ook een eigenvector van $L$.
\[
L(v_{k+1}) = \lambda_{k+1}v_{k+1} = \lambda_{k+1}\left(\sum_{i=1}^k\mu_iv_i\right) = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
Voegen we deze twee nu samen dan krijgen we het volgende.
\[
\sum_{i=1}^k\mu_i\lambda_iv_i = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
\[
\sum_{i=1}^k \mu_i(\lambda_i-\lambda_{k+1})v_i = 0
\]
Omdat we weten dat alle $\lambda_i$ onderling verschillend zijn (gegeven) en dat alle $v_i$ lineair onafhankelijk zijn houdt dit in dat alle $\mu_i$ nul moeten zijn. Dit zou betekenen dat alle $v_i$ lineair onafhankelijk zijn en dat is in contradictie met de aanname van lineair afhankelijkheid.
\end{proof}

\subsection{Gevolg 5.20 p 191}
Zij $L$ een lineaire transformatie van de $n$-dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ heeft een enkelvoudig spectrum $\Rightarrow$ $L$ is diagonaliseerbaar.
\end{center}

\subsubsection*{Bewijs}
\begin{proof}
Neem voor elke eigenwaarde van $L$ de bijhorende eigenvector
Elk van die eigenvectoren zijn lineair onafhankelijk\footnote{Zie Stelling 5.18 190}.
Omdat $L$ een enkelvoudig spectrum heeft zijn dit precies $n$ eigenvectoren.
Deze vectoren zijn bijgevolg ook voortbrengend voor $V$ en een basis van $V$\footnote{Zie Stelling 3.41 p 109}.
Nu voldoet $L$ aan de definitie van een diagonaliseerbare lineaire transformatie.
\end{proof}


\subsection{Lemma 5.22 p 192}
Zij $L$ een diagonaliseerbare lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
De karakteristieke veelterm van $L$ is volledig te ontbinden als product van eerstegraadsfactoren.

\subsubsection*{Bewijs}
\begin{proof}
Omdat $L$ diagonaliseerbaar is bestaat er een inverteerbare $P$ zodat $B$ een diagonaalmatrix is in $L = P^{-1}BP$ \footnote{Zie het bewijs van Stelling 5.7 p 181}. Omdat de karakteristieke veelter onafhankelijk is van de basis is deze voor $L_A$ en $L_B$ gelijk \footnote{Zie Gevolg 5.9 p 182}. De karakteristieke veelterm van $L_B$ is dus $\phi_L = \prod_{i=1}^n(X-\lambda_i)$ met $\lambda_i$ de waarden op de diagonaal van $B$ (dit zijn de eigenwaarden van $L$).  $\phi_L$ is bijgevolg een product van eerstegraadsvectoren.
\end{proof}


\subsection{Stelling 5.23 p 192}
Zij $(\mathbb{R},V,+)$ een vectorruimte van dimensie $n$ en zij $L$ een lineaire transformatie van $V$ waarvan de karakteristieke veelterm $\phi_L$ volledig te ontbinden valt als product van eerstegraadsfactoren. Zij $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_n\}$ het spectrum van $L$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ is diagonaliseerbaar.
\end{center}
\[\Leftrightarrow\]
\[
\forall \text{ eigenwaarde } \lambda_i: d(\lambda_i) = m(\lambda_i)
\]

\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
We weten dat er een basis $\beta$ van $V$ bestaat die volledig uit eigenvectoren van $L$ bestaat omdat $L$ diagonaliseerbaar is\footnote{Zie Definitie 5.6 p 181}. Rangschik de vectoren in $\beta$ nu volgens eigenwaarde. Per eigenwaarde zijn er dus $m(\lambda)$ vectoren. De vectoren die bij dezelfde eigenwaarde horen vormen een basis voor de eigenruimte van die eigenwaarde. Per eigenwaarde zijn er dus $d(\lambda)$ vectoren. Voor elke eigenwaarden geldt dus de volgende bewering.
\[
d(\lambda_i) = m(\lambda_i)
\]
Dit is gemakkelijk te zien als we matrixvoorstelling van $L$ ten opzichte van de eigenbasis opstellen.
\[
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 & 0 & \cdots & 0 & \cdots & 0\\
0 & \lambda_1 & \cdots & 0 & 0 & \cdots & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots&\vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_1& 0 & \cdots   &0 & \cdots & 0\\
0 & 0 & \cdots & 0 & \lambda_2 & \cdots  &0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots&\vdots\\
0 & 0 & \cdots & 0 & 0 & \cdots & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots&\vdots\\
0 & 0 & \cdots & 0 & 0 & \cdots & 0 & \cdots & \lambda_n\\
\end{pmatrix}
\]
Zoals we zien komt $\lambda_i$ precies $m(\lambda_i)$ keer voor en komen er met $\lambda_i$ precies $d(\lambda_i)$ eigenvectoren voor.

\item $\Leftarrow$\\
Voor elke eigenwaarde geldt $d(\lambda_i) = m(\lambda_i) = m_i$.
Omdat de karakteristieke veelterm volledig als product kan ontbonden worden in eerstegraadsfactoren en de karakteristieke veelterm van graag $n$ is (omdat de afmetingen van $L_A$ $n\times
n$ zijn) geldt de volgende bewering.
\[
\sum_{i=1}^km_i = n
\]
We kiezen nu voor elke eigenruimte $E_{\lambda_i}$ een basis $\beta_{E_{\lambda_i}} = \{v_{i,1},v_{i,2},...,v_{i,m_i}\}$. Zetten we nu al deze vectoren samen dan krijgen we de volgende verzameling.
\[
\beta = 
\{
v_{1,1},v_{1,2},...,v_{1,m_i},v_{2,1},v_{2,2},...,v_{2,m_i},v_{k,1},v_{k,2},...,v_{k,m_i}
\}
\]
Omdat $\sum_{i=1}^km_i = n$ geldt bevat deze verzameling $\beta$ precies $n$ elementen.
Als we nu kunnen bewijzen dat $\beta$ een basis vormt voor $V$ dan heeft $V$ dus een basis bestaande uit eigenvectoren en is $L$ bijgevolg diagonaliseerbaar. We zullen aantonen dat $\beta$ een vrij deel is van $V$, daaruit volgt dat $\beta$ een basis vormt voor $V$ \footnote{Zie Stelling 3.41 p 109}.\\\\
Stel nu dat er een lineaire combinatie van de vectoren in $\beta$ de nulvector oplevert.
\[
\sum_{i=1}^n\mu_{i,j}\beta_i = \vec{0}
\]
Omdat $\beta_i$ basissen zijn van de eigenruimten $E_{\lambda_i}$ geldt dat de volgende uitdrukking ook een vector is uit de eigenruimte $E_{\lambda_i}$ die bij eigenwaarde $i$ geldt. Dit betekent \emph{niet} per se dat $u_i$ eigenvectoren zijn, het kunnen namelijk ook nulvectoren zijn. Dit gaan we nu gebruiken.
\[
\sum_{j=1}^{m_i} \mu_{i,j}v_{i,j} = u_i
\]
De lineaire combinatie kan dus ook geschreven worden als volgt.
\[
\sum_{i=1}^ku_i = \vec{0}
\]
Alle $u_i$ in deze uitdrukking horen bij \emph{verschillende} eigenwaarden. Dit betekent dat de $u_i$ lineair onafhankelijk zijn \footnote{Zie stelling 5.18 p 190.}. $u_i$ kunnen enkel lineair onafhankelijk zijn als alle $u_i$ nulvectoren zijn.
%Hier zit iets raar, lineair onafhankelijke nulvectoren? TODO
\[
u_1 = u_2 = ... = u_k
\]
Nu geldt voor elke $i$ dus het volgende.
\[
\sum_{j=1}^{m_i} \mu_{i,j}v_{i,j} = \vec{0}
\]
Omdat de $v_{i,j}$ basisvectoren van de eigenruimten $E_{\lambda_i}$ geldt dat alle $\mu_{i,j}$ nul moeten zijn.
\end{itemize}
\end{proof}

\subsection{Opmerking 5.24 p 193}
Zij $L$ een diagonaliseerbare lineaire transformatie van $(\mathbb{R},V,+)$. 

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $V$ kan geschreven worden als volgt.
\[
V = \sum_{i=1}^kE_{\lambda_i} = \oplus E_{\lambda_{i}}
\]

\item
De (disjunctie) unie van basissen van de $E_{\lambda_{i}}$ vormt een basis van $V$.

\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Omdat $L$ diagonaliseerbaar is heeft $V$ een basis $\beta=\{\beta_1,\beta_2,...,\beta_n\} $ die volledig uit basisvectoren van $L$ bestaat \footnote{Zie Definitie 5.6 p 181.}.
Noem $\lambda_i$ de eigenwaarde die bij eigenvector $\beta_i$ hoort en $E_{\lambda_i}$ de eigenruimte die bij $\lambda_i$ hoort.

\begin{enumerate}
\item
We groeperen de eigenvectoren in $\beta$ per eigenwaarde.
Uit de vorige stelling weten we nu dat elk van deze groepjes een basis vormen voor de respectievelijke eigenruimten $E_{\lambda_i}$.
Omdat $\beta$ een basis is van $V$ en elke groepje een basis is van $E_{\lambda_i}$ geldt nu dat elke vector $v\in V$ geschreven kan worden als een som van een vector in elke eigenruimte. 
Nu rest er ons nog te bewijzen dat deze som uniek is.
We moeten daarvoor enkel nog aantonen dat de doorsnede van de eigenruimten elke keer enkel de nulvector bevat \footnote{Zie Propositie 3.22 p 99.}.\\

Stel dat \'e\'en van die doorsneden meer dan enkel de nulvector bevat, zeg, nog een vector $v$.
Dan zou dit betekenen dat $v$ een eigenvector is die bij twee verschillende eigenwaarden hoort.
Dit kan niet omdat eigenvectoren in verschillende eigenwaarden lineair onafhankelijk zijn voor diagonaliseerbare lineaire transformaties \footnote{Zie Stelling 5.18 p 190.}.\\\\

$V$ is dus de directe som van alle eigenruimten van $L$.

\item
Noem $m_i$ de algebra\"ische multipliciteit van eigenwaarde $i$ van $L$.
Omdat de karakteristieke veelterm van $L$ zich volledig ontbindt als product van eerstegraadsfactoren geldt de volgende gelijkheid waarbij $k$ het aantal eigenruimten van $L$ is.
\[
\sum_{i=1}^km_i = n
\]
Nemen we de unie van de basissen van de eigenruimten dan vormt deze unie opnieuw een basis van $V$. Voor meer uitleg zie het bewijs van Stelling 5.23 p 192.
\end{enumerate}
\end{proof}


\subsection{Propositie 5.25 p 194}
Zij $L$ een lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$ waarvan de karakteristieke veelterm volledig te ontbinden valt als product van eerstegraadsfactoren.

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item De determinant van $L$ is gelijk aan het product van de eigenwaarden van $L$.
\item Het spoor van $L$ is gelijk aan de som van de eigenwaarden van $L$.
\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.
\begin{enumerate}
\item
Zij $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_k\}$ het spectrum van $L$.
Zij $m_i = m(\lambda)$ de algebra\"ische multipliciteit van eigenwaarde $i$.
De karakteristieke veelterm van $L$ ziet er nu als volgt uit.
\[
\phi_L(X) = \prod_{i=1}^k(X-\lambda_i)^{m_i}
\]
Wanneer we dit product uitwerken ziet het er als volgt uit.
%TODO meer uitleg.
\[
X^n + (-1)(m_1\lambda_1 + m_2\lambda_2 + ... + m_3\lambda_3)X^{n-1} + (-1)^2(m_1\lambda_1 + m_2\lambda_2 + ... + m_3\lambda_3)^2X^{n-2} + ... + (-1)^n\prod_{i=1}^k\lambda_i^{m_i}
\]
Per defintie is de karakteristieke veelterm ook gelijk aan het volgende.
In deze formule is $A$ de matrix van $L$ ten opzichte van eender welke basis van $V$.
\[
\phi_L(X) = det(X\mathbb{I}_n-A))
\]
%TODO Hier gebeurt iets fishy, wtf?? Het is de eerste term van de bovenstaande uitdrukking, maar wut?
\[
(-1)^n\prod_{i=1}^k\lambda_i^{m_i} = \prod_{i=1}^k(-\lambda_i)^{m_i} = \phi_L(0) = det(-A) = (-1)^ndet(A) = (-1)^ndet(L) %Waar komt die laatste stap ineens vandaan??
\]
$det(L)$ is dus precies gelijk aan het product van alle eigenwaarden van $L$ waarbij elke eigenwaarde $\lambda_i$ $m_i$ keer geteld wordt.

\item
We beschouwen $\phi_L$ opnieuw op beide manieren.
\[
\phi_L = \prod_{i=1}^k(X-\lambda_i)^{m_i} \text{ en }\phi_L = det(X\mathbb{I}_n - A)
\]
In beide schrijfwijzen beschouwen we enkel de term bij co\"effici\"ent $X^{n-1}$.
Deze moeten gelijk zijn.
\[
-\sum_{i=1}^km_i\lambda_i = -\sum_{k=1}^n(A)_{ii} = -Tr(A) = -Tr(L)
\]
Het spoor van $L$ is dus gelijk aan de som van alle eigenwaarden $\lambda_i$ waarin elke eigenwaarde $m_i$ keer geteld wordt.
\end{enumerate}
\end{proof}

\subsection{Stelling 5.28 p 203}
We zullen hiervoor geen bewijs geven. Dit moet je ook niet kunnen, maar zorg wel dat je goed begrijpt wat deze stelling inhoudt.
%TODO Eventueel nog meer uitleg?

\subsection{Stelling 5.31 p 205}
Dit bewijzen we op dezelfde manier als Stelling 5.23 p 192

\subsection{Propositie 5.32 p 205}
Dit bewijzen we op dezelfde manier als Stelling 5.25 p 194

\subsection{Propositie 5.38 p 209}
Zij $X = (x_1 x_2 \cdots x_n)^T \in \mathbb{R}^n$ een kansvector en zijn $M$ een stochastische $n\times n$ matrix.
\subsubsection*{Te Bewijzen}
\[
M\cdot X \text{ is een kansvector.}
\]
\subsubsection*{Bewijs}
\begin{proof}
In een stochastische matrix en in een kansvector zijn alle elementen positief. Elk element van $M\cdot X$ is dus ook positief. Nu rest er ons nog te bewijzen dat de som van de elementen in $M \cdot X$ gelijk is aan $1$.
\end{proof}
\[
\sum_{i=1}^n(M\cdot X)_i = \sum_{i=1}^n\sum_{j=1}^nm_{ij}x_j= \sum_{j=1}^nx_j(\sum_{i=1}^nm_{ij})
\]
Let in bovenstaande vergelijking goed op de sommatie tekens. Voor de tweede gelijkheid wisselen we eerst de sommatietekens om. Dit mag omwille van de commutatitviteit van de optelling in $\mathbb{R}$. Daarna zonderen we de sommen van kolommen af. We weten dat die precies elke keer $1$ zijn omdat $M$ een stochastische matrix is.
\[
\sum_{j=1}^nx_j(\sum_{i=1}^nm_{ij}) = \sum_{j=1}^nx_j = 1
\]
De laatste gelijkheid geldt omdat $X$ een kansvector is, en de som van de elementen van $X$ bijgevolg precies $1$ is.

\subsection{Gevolg 5.39 p 209}
Zij alle $X_0$ een kansvector en $M$ een stochastische matrix.
\subsubsection*{Te Bewijzen}
Een markov keten bestaat uit een rij kansvectoren. Alle $X_i$ zijn dus kansvectoren.
\[
X0, X_1 = M\cdot X_0, X_2 = M^2\cdot X_0,...,X_n = M^n\cdot X_0
\]
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door inductie op $n$.\\\\
\emph{Stap 1: (Basis stap)}\\
De bewering geldt voor $n=1$ \footnote{Zie Propositie 5.38 p 209}.\\\\
\emph{Stap 2: (Inductie stap}\\
Stel dat de bewering geldt voor een bepaalde $n=k$, dan bewijzen we nu dat daaruit volgt dat de bewering geldt voor $n=k+1$.
We moeten nu bewijzen dat de volgende vector een kansvector is.
\[
M^{k+1}\cdot X_0 = M^{k}\cdot M\cdot X_0
\]
Volgens de basisstap geldt dat $M\cdot X_0$ een kansvector is. Volgens de inductiehypothese geldt dat $M^{k}\cdot (M\cdot X_0)$ een kansvector is. Bijgevolg is $M^{k+1}\cdot X_0$ een kansvector.
\end{proof}

\subsection{Propositite 5.40 p 209}
Zij $M\in \mathbb{R}^{n\times n}$ een stochastische matrix.
\subsubsection*{Te Bewijzen}
Het getal $1$ is een eigenwaarde van $M$
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.\\
Beschouw het stelsel $M\cdot X = \lambda X$ met $\lambda = 1$. De volgende bewering is hieraan equivalent.
\[
(M-\mathbb{I}_n)\cdot X = \vec{0}
\]
Omdat $M$ een stochastische matrix is geldt dat de som van de elementen van elke kolom gelijk is aan $1$. Daaruit volgt dat de som van de elementen van elke kolom in $M-\mathbb{I}_n$ gelijk is aan $0$. Als we de rijen van $M$ optellen krijgen we een nulrij. Dit houdt in dat de rijen van $M-\mathbb{I}_n$ lineair afhankelijk zijn \footnote{Zie Stelling 2.2 p 57 D-5 en Steling 2.3 p 58 1.} en dat de volgende bewering geldt.
\[
det(M - \mathbb{I}_n) = det(M - \lambda\mathbb{I}_n) = 0
\]
Dit betekent precies dat $1$ een eigenwaarde is van $M$.
(De eigenruimte $E_1$ bestaat uit de vaste punten van de overgang (vandaar de naam vaste vector). M.a.w. de $X$ waarvoor geldt dat $M\cdot X = X$).
\end{proof}


\subsection{Propositie 5.41 p 210}
Zij $M\in \mathbb{R}^{n\times n}$ een stochastische matrix waarvan alle elementen verschillend zijn van nul.
\subsubsection*{Te Bewijzen}
De eigenruimte $E_1$ bestaat, heeft dimensie $1$ en bevat precies $1$ kansvector.

\subsubsection*{Bewijs}
\begin{proof}
\begin{itemize}
\item De eigenruimte $E_1$ bestaat \footnote{Zie Propositie 5.40 p 209}. 
\item De eigenruimte $E_1$ heeft dimensie $1$ want we kunnen elke vaste vector van $M$ nemen en door lineaire combinatie ervan elke andere vaste vector van $M$ vormen. Zij $X$ een vaste vector en $\lambda \in \mathbb{R}$ een scalar, dan geldt voor elke $\lambda$ dat $\lambda X$ ook een vaste vector is.
\[
M\cdot X = X\\ \Leftrightarrow M\cdot \lambda X = \lambda X 
\]
(Dit is makkelijk in te zien als we $M$ als een lineaire afbeelding beschouwen.)
\item Stel dat er meer dan \'e\'en vaste vector van $M$ bestaat die ook een kansvector is, noem ze $X$ en $Y$.
Dan geldt dat er een $\lambda 1$ bestaat zodat $\lambda X = Y$ want we kunnen $X$ als de basis voor $E_1$ nemen. De som van de elementen van $Y$ is dus $\lambda$ keer die van $X$. Dit zou betekenen dat $\lambda$ $1$ is omdat $\lambda X = Y$ en de som van de elementen van $Y$ $1$. Dit is in contradictie met de aanname dat $X$ verschillend is van $Y$.
\end{itemize}
\end{proof}

\end{document}